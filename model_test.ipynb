{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from json import load\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import pkbar\n",
    "import sys\n",
    "import pkbar\n",
    "from unet3d.config import *\n",
    "from tqdm import tqdm\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.nn.functional import one_hot\n",
    "from torch.optim import Adam\n",
    "from unet3d.unet3d_vgg16 import UNet3D_VGG16\n",
    "from utils.Other import get_headers\n",
    "from unet3d.dataset import SAIADDataset, WrappedDataLoader, to_device\n",
    "from torch.utils.data import DataLoader\n",
    "from pynvml.smi import nvidia_smi\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "torch.backends.cudnn.benchmark = True # Speeds up stuff\n",
    "torch.backends.cudnn.enabled = True\n",
    "device = torch.device('cuda')\n",
    "nvsmi = nvidia_smi.getInstance()\n",
    "\n",
    "_,_,patient_names = get_headers(DATASET_PATH)\n",
    "\n",
    "### FOR TESTING ###\n",
    "#TRAIN_BATCHES_PER_EPOCH=20\n",
    "#VAL_BATCHES_PER_EPOCH=5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with val patients: ['SAIAD 15', 'SAIAD 11']\n",
      "Fetching patients probabilities...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 20/20 [00:06<00:00,  3.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching patients probabilities...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 2/2 [00:00<00:00,  2.41it/s]\n",
      "/Work/Users/acharneca/.conda/envs/saiad-pytorch/lib/python3.8/site-packages/torch/utils/data/dataloader.py:563: UserWarning: This DataLoader will create 2 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    }
   ],
   "source": [
    "excl_patients_training = ['SAIAD 15', 'SAIAD 11'] #patients for validation/testing\n",
    "excl_patients_val = list(set(patient_names) - set(excl_patients_training))\n",
    "\n",
    "print(\"Training with val patients:\", excl_patients_training)\n",
    "\n",
    "\n",
    "\n",
    "## Load dataset ##\n",
    "train_dataset = SAIADDataset(\n",
    "    excl_patients=excl_patients_training,\n",
    "    load_data_to_memory=True,\n",
    "    n_batches=TRAIN_BATCHES_PER_EPOCH,\n",
    "    )\n",
    "val_dataset = SAIADDataset(\n",
    "    excl_patients=excl_patients_val,\n",
    "    load_data_to_memory=True,\n",
    "    n_batches=VAL_BATCHES_PER_EPOCH,\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=TRAIN_BATCH_SIZE,\n",
    "    shuffle=False, \n",
    "    pin_memory=True, \n",
    "    num_workers=NUM_WORKERS\n",
    "    )\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=VAL_BATCH_SIZE,\n",
    "    shuffle=False, \n",
    "    pin_memory=True, \n",
    "    num_workers=NUM_WORKERS\n",
    "    )\n",
    "\n",
    "val_dataloader = WrappedDataLoader(val_dataloader, to_device, device)\n",
    "train_dataloader = WrappedDataLoader(train_dataloader, to_device, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. Usage - Used: 1231.3125/16160.5 MB\n",
      "Epoch: 1/100\n",
      "50/50 [========] - 87s 2s/step - loss: 0.1997 - Validation loss: 0.1653\n",
      "\t Validation Loss Decreased(inf--->0.165269) \t Saving The Model\n",
      "Mem. Usage - Used: 15173.3125/16160.5 MB\n",
      "Epoch: 2/100\n",
      "50/50 [========] - 76s 2s/step - loss: 0.1599 - Validation loss: 0.1635\n",
      "\t Validation Loss Decreased(0.165269--->0.163460) \t Saving The Model\n",
      "Mem. Usage - Used: 15435.3125/16160.5 MB\n",
      "Epoch: 3/100\n",
      "50/50 [========] - 76s 2s/step - loss: 0.1378 - Validation loss: 0.1682\n",
      "Mem. Usage - Used: 15435.3125/16160.5 MB\n",
      "Epoch: 4/100\n",
      "50/50 [========] - 79s 2s/step - loss: 0.1275 - Validation loss: 0.1692\n",
      "Mem. Usage - Used: 15435.3125/16160.5 MB\n",
      "Epoch: 5/100\n",
      "50/50 [========] - 76s 2s/step - loss: 0.1292 - Validation loss: 0.1600\n",
      "\t Validation Loss Decreased(0.163460--->0.160028) \t Saving The Model\n",
      "Mem. Usage - Used: 15435.3125/16160.5 MB\n",
      "Epoch: 6/100\n",
      "50/50 [========] - 76s 2s/step - loss: 0.1283 - Validation loss: 0.1662\n",
      "Mem. Usage - Used: 15435.3125/16160.5 MB\n",
      "Epoch: 7/100\n",
      "50/50 [========] - 76s 2s/step - loss: 0.1141 - Validation loss: 0.1714\n",
      "Mem. Usage - Used: 15435.3125/16160.5 MB\n",
      "Epoch: 8/100\n",
      "50/50 [========] - 76s 2s/step - loss: 0.1221 - Validation loss: 0.1666\n",
      "Mem. Usage - Used: 15435.3125/16160.5 MB\n",
      "Epoch: 9/100\n",
      "50/50 [========] - 76s 2s/step - loss: 0.1175 - Validation loss: 0.1678\n",
      "Mem. Usage - Used: 15435.3125/16160.5 MB\n",
      "Epoch: 10/100\n",
      "50/50 [========] - 76s 2s/step - loss: 0.1042 - Validation loss: 0.1614\n",
      "Mem. Usage - Used: 15435.3125/16160.5 MB\n",
      "Epoch: 11/100\n",
      "50/50 [========] - 76s 2s/step - loss: 0.1065 - Validation loss: 0.1654\n",
      "Mem. Usage - Used: 15435.3125/16160.5 MB\n",
      "Epoch: 12/100\n",
      "50/50 [========] - 76s 2s/step - loss: 0.0983 - Validation loss: 0.1668\n",
      "Mem. Usage - Used: 15435.3125/16160.5 MB\n",
      "Epoch: 13/100\n",
      "50/50 [========] - 76s 2s/step - loss: 0.1011 - Validation loss: 0.1650\n",
      "Mem. Usage - Used: 15435.3125/16160.5 MB\n",
      "Epoch: 14/100\n",
      "50/50 [========] - 76s 2s/step - loss: 0.0921 - Validation loss: 0.1635\n",
      "Mem. Usage - Used: 15435.3125/16160.5 MB\n",
      "Epoch: 15/100\n",
      "50/50 [========] - 76s 2s/step - loss: 0.0951 - Validation loss: 0.1629\n",
      "Mem. Usage - Used: 15435.3125/16160.5 MB\n",
      "Epoch: 16/100\n",
      "50/50 [========] - 76s 2s/step - loss: 0.0877 - Validation loss: 0.1643\n",
      "Mem. Usage - Used: 15435.3125/16160.5 MB\n",
      "Epoch: 17/100\n",
      "50/50 [========] - 76s 2s/step - loss: 0.0852 - Validation loss: 0.1643\n",
      "Mem. Usage - Used: 15435.3125/16160.5 MB\n",
      "Epoch: 18/100\n",
      "50/50 [========] - 76s 2s/step - loss: 0.0837 - Validation loss: 0.1710\n",
      "Mem. Usage - Used: 15435.3125/16160.5 MB\n",
      "Epoch: 19/100\n",
      "50/50 [========] - 76s 2s/step - loss: 0.0746 - Validation loss: 0.1693\n",
      "Mem. Usage - Used: 15435.3125/16160.5 MB\n",
      "Epoch: 20/100\n",
      "50/50 [========] - 76s 2s/step - loss: 0.0720 - Validation loss: 0.1654\n",
      "Mem. Usage - Used: 15435.3125/16160.5 MB\n",
      "Epoch: 21/100\n",
      "50/50 [========] - 76s 2s/step - loss: 0.0699 - Validation loss: 0.1667\n",
      "Mem. Usage - Used: 15435.3125/16160.5 MB\n",
      "Epoch: 22/100\n",
      "50/50 [========] - 76s 2s/step - loss: 0.0700 - Validation loss: 0.1667\n",
      "Mem. Usage - Used: 15435.3125/16160.5 MB\n",
      "Epoch: 23/100\n",
      "50/50 [========] - 76s 2s/step - loss: 0.0717 - Validation loss: 0.1702\n",
      "Mem. Usage - Used: 15435.3125/16160.5 MB\n",
      "Epoch: 24/100\n",
      "50/50 [========] - 76s 2s/step - loss: 0.0606 - Validation loss: 0.1721\n",
      "Mem. Usage - Used: 15435.3125/16160.5 MB\n",
      "Epoch: 25/100\n",
      "50/50 [========] - 76s 2s/step - loss: 0.0599 - Validation loss: 0.1705\n",
      "Mem. Usage - Used: 15435.3125/16160.5 MB\n",
      "Epoch: 26/100\n",
      "50/50 [========] - 76s 2s/step - loss: 0.0649 - Validation loss: 0.1637\n",
      "Mem. Usage - Used: 15435.3125/16160.5 MB\n",
      "Epoch: 27/100\n",
      "50/50 [========] - 76s 2s/step - loss: 0.0609 - Validation loss: 0.1713\n",
      "Mem. Usage - Used: 15435.3125/16160.5 MB\n",
      "Epoch: 28/100\n",
      "50/50 [========] - 76s 2s/step - loss: 0.0564 - Validation loss: 0.1712\n",
      "Mem. Usage - Used: 15435.3125/16160.5 MB\n",
      "Epoch: 29/100\n",
      "50/50 [========] - 76s 2s/step - loss: 0.0519 - Validation loss: 0.1706\n",
      "Mem. Usage - Used: 15435.3125/16160.5 MB\n",
      "Epoch: 30/100\n",
      "50/50 [========] - 76s 2s/step - loss: 0.0526 - Validation loss: 0.1725\n",
      "Mem. Usage - Used: 15435.3125/16160.5 MB\n",
      "Epoch: 31/100\n",
      "50/50 [========] - 76s 2s/step - loss: 0.0457 - Validation loss: 0.1783\n",
      "Mem. Usage - Used: 15435.3125/16160.5 MB\n",
      "Epoch: 32/100\n",
      "50/50 [========] - 76s 2s/step - loss: 0.0494 - Validation loss: 0.1777\n",
      "Mem. Usage - Used: 15435.3125/16160.5 MB\n",
      "Epoch: 33/100\n",
      "50/50 [========] - 76s 2s/step - loss: 0.0454 - Validation loss: 0.1699\n",
      "Mem. Usage - Used: 15435.3125/16160.5 MB\n",
      "Epoch: 34/100\n",
      "50/50 [========] - 76s 2s/step - loss: 0.0438 - Validation loss: 0.1790\n",
      "Mem. Usage - Used: 15435.3125/16160.5 MB\n",
      "Epoch: 35/100\n",
      "50/50 [========] - 76s 2s/step - loss: 0.0421 - Validation loss: 0.1721\n",
      "Mem. Usage - Used: 15435.3125/16160.5 MB\n",
      "Epoch: 36/100\n",
      "50/50 [========] - 76s 2s/step - loss: 0.0427 - Validation loss: 0.1818\n",
      "Mem. Usage - Used: 15435.3125/16160.5 MB\n",
      "Epoch: 37/100\n",
      "50/50 [========] - 76s 2s/step - loss: 0.0405 - Validation loss: 0.1783\n",
      "Mem. Usage - Used: 15435.3125/16160.5 MB\n",
      "Epoch: 38/100\n",
      "40/50 [=====>..] - ETA: 16s - loss: 0.0371"
     ]
    }
   ],
   "source": [
    "## Model ##\n",
    "model = UNet3D_VGG16(\n",
    "    in_channels=IN_CHANNELS , \n",
    "    num_classes=NUM_CLASSES,\n",
    "    use_softmax_end=False #set this to false for training with CELoss\n",
    "    ).to(device)\n",
    "\n",
    "loss_fn = CrossEntropyLoss(weight=torch.Tensor(np.array(CE_WEIGHTS)/np.array(CE_WEIGHTS).sum())).cuda()\n",
    "optimizer = Adam(params=model.parameters(), lr=LR)\n",
    "\n",
    "\n",
    "\n",
    "## Training ##\n",
    "min_valid_loss = math.inf\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # Check memory usage\n",
    "    mem_query = nvsmi.DeviceQuery('memory.free, memory.total')['gpu'][0]['fb_memory_usage']\n",
    "    print(f\"Mem. Usage - Used: {mem_query['total']-mem_query['free'] }/{mem_query['total']} MB\")\n",
    "\n",
    "    # progress bar\n",
    "    kbar = pkbar.Kbar(target=TRAIN_BATCHES_PER_EPOCH+VAL_BATCHES_PER_EPOCH, epoch=epoch, num_epochs=EPOCHS, width=8, always_stateful=True)\n",
    "\n",
    "    train_loss = 0.0\n",
    "    model.train()\n",
    "    i=1\n",
    "    batch_num = 1\n",
    "    for X_batch, y_batch in train_dataloader:  \n",
    "        pred = model(X_batch)\n",
    "        loss = loss_fn(pred, y_batch)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.cpu().detach()/batch_num\n",
    "        kbar.update(i, values=[(\"loss\", train_loss)])\n",
    "        i+=1\n",
    "        batch_num+=1\n",
    "        \n",
    "    # Tensorboard #\n",
    "    #writer.add_scalar(\"Loss/train\", train_loss, epoch)\n",
    "\n",
    "    valid_loss = 0.0\n",
    "    model.eval()\n",
    "    batch_num = 1\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_dataloader:\n",
    "            pred = model(X_batch)\n",
    "            loss = loss_fn(pred,y_batch)\n",
    "            valid_loss += loss.cpu().detach()/batch_num\n",
    "            kbar.update(i, values=[(\"Validation loss\", valid_loss)])\n",
    "            i+=1\n",
    "            batch_num+=1\n",
    "\n",
    "            \n",
    "    # Tensorboard #\n",
    "    #writer.add_scalar(\"Loss/val\", valid_loss, epoch)\n",
    "\n",
    "    if min_valid_loss > valid_loss:\n",
    "        print(f'\\t Validation Loss Decreased({min_valid_loss:.6f}--->{valid_loss:.6f}) \\t Saving The Model')\n",
    "        min_valid_loss = valid_loss\n",
    "        # Saving State Dict\n",
    "        torch.save(model.state_dict(), f'checkpoints/no_aug_epoch{epoch}_valLoss{min_valid_loss:.6f}.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbdb05a012d341849458033d0683131f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(RadioButtons(description='Slice plane selection:', options=('x-y', 'y-z', 'z-x'), style=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<utils.Visualization.ImageSliceViewer3D at 0x151724f16a90>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.Visualization import ImageSliceViewer3D\n",
    "n=70\n",
    "pred = model(scan_patches[n:n+1].cuda())\n",
    "pred.size()\n",
    "pred_index = np.array(torch.argmax(pred[0].cpu(), dim=0))\n",
    "ImageSliceViewer3D(pred_index, np.array(scan_patches[n:n+1][0,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4688, device='cuda:0', grad_fn=<DivBackward1>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn(pred, segm_patches[n:n+1].cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:0',\n",
       "       grad_fn=<Unique2Backward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.unique(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 36 125  77  99  75  91  95 110  57 102  11 111  80  97  40  17 126  53\n",
      "   2  54  30 123 116  28  23  93  81  88  49  46  96  62  90 124  61   3\n",
      "   7  86  34   6  55  84 109   9  20   5  78  67 112  64  25  44  10  92\n",
      "  66  98 108  59  41  50  24  63 117  76 104 120  56   8  26  43  33  37\n",
      "  89   4 106  14   1  35 107 114   0  45  94  69 100  83  18  60  12 119\n",
      "  70  51  31  87  16  22 115  13  38 127  42  68  74 118 103 121 101 122\n",
      "  15  19  85  73  58  29  21  52 105  39  82  79  47  71 113  72  48  65\n",
      "  27  32]\n"
     ]
    }
   ],
   "source": [
    "idx = np.arange(128)\n",
    "np.random.shuffle(idx)\n",
    "print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 128/128 [00:01<00:00, 121.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.Size([128, 1, 96, 96, 96]), torch.float32]\n",
      "[torch.Size([128, 5, 96, 96, 96]), torch.float32]\n"
     ]
    }
   ],
   "source": [
    "# Load some data and uniformly sample from it\n",
    "scan, _ = nrrd.read(DATASET_PATH + '/SAIAD 1/scan.nrrd')\n",
    "segm, _ = nrrd.read(DATASET_PATH + '/SAIAD 1/segm.nrrd')\n",
    "\n",
    "#scan_patches = patchify(scan, PATCH_SIZE, step=PATCH_SIZE).reshape(-1,PATCH_SIZE[0],PATCH_SIZE[1], PATCH_SIZE[2])\n",
    "\n",
    "## Random Sampling: Uniform\n",
    "scan_patches = []\n",
    "segm_patches = []\n",
    "side_len = PATCH_SIZE[0]\n",
    "for i in tqdm(range(128)):\n",
    "    # Center coordinates\n",
    "    cx = torch.randint(0,scan.shape[0],(1,))[0]\n",
    "    cy = torch.randint(0,scan.shape[1],(1,))[0]\n",
    "    cz = torch.randint(0,scan.shape[2],(1,))[0]\n",
    "    \n",
    "    #print(f\"Center: {[cx,cy,cz]}\")\n",
    "    bbox_x = [max(cx - side_len//2, 0), min(scan.shape[0], cx+side_len//2)]\n",
    "    bbox_y = [max(cy - side_len//2, 0), min(scan.shape[1], cy+side_len//2)]\n",
    "    bbox_z = [max(cz - side_len//2, 0), min(scan.shape[2], cz+side_len//2)]\n",
    "\n",
    "    # Random patch\n",
    "    pad_x = (-min(cx - side_len//2,0), max(side_len//2 + cx - scan.shape[0], 0))\n",
    "    pad_y = (-min(cy - side_len//2,0), max(side_len//2 + cy - scan.shape[1], 0))\n",
    "    pad_z = (-min(cz - side_len//2,0), max(side_len//2 + cz - scan.shape[2], 0))\n",
    "    \n",
    "    #print([pad_x, pad_y, pad_z])\n",
    "\n",
    "    segm_patch_prepad = segm[bbox_x[0]:bbox_x[1], bbox_y[0]:bbox_y[1], bbox_z[0]:bbox_z[1]]\n",
    "    scan_patch_prepad = scan[bbox_x[0]:bbox_x[1], bbox_y[0]:bbox_y[1], bbox_z[0]:bbox_z[1]]\n",
    "    scan_patch = np.pad(scan_patch_prepad,(pad_x, pad_y, pad_z), 'constant', constant_values=0)\n",
    "    segm_patch = np.pad(segm_patch_prepad,(pad_x, pad_y, pad_z), 'constant', constant_values=0)\n",
    "    \n",
    "    scan_patches.append(scan_patch)\n",
    "    segm_patches.append(segm_patch)\n",
    "    \n",
    "scan_patches = torch.tensor(np.array(scan_patches)).float()\n",
    "scan_patches = torch.unsqueeze(scan_patches,1) # add channel dimension, send to gpu\n",
    "segm_patches = np.array(segm_patches).reshape(-1,PATCH_SIZE[0],PATCH_SIZE[1], PATCH_SIZE[2])\n",
    "segm_patches = one_hot(torch.tensor(segm_patches).to(torch.int64), num_classes=NUM_CLASSES).permute(0,4,1,2,3).float()# put channels first, send to gpu\n",
    "\n",
    "print([scan_patches.shape, scan_patches.dtype])\n",
    "print([segm_patches.shape, segm_patches.dtype])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "32aad0a8508bff835c20a8d47234734688fb62c0cbdf7488c1eae1822cd413fe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
